# ğŸ§  Digit Recognizer â€“ Dimensionality Reduction + ML/DL Exploration

[![Kaggle Competition](https://img.shields.io/badge/Kaggle-Digit%20Recognizer-20BEFF?logo=kaggle&logoColor=white)](https://www.kaggle.com/competitions/digit-recognizer/overview)

---

## ğŸ“Œ Competition Overview

### ğŸ¯ Goal

Your objective is to classify grayscale images of hand-drawn digits (0â€“9). The dataset used is the **classic MNIST** datasetâ€”commonly known as the *"Hello World"* of computer vision.

Youâ€™ll build models to correctly predict the digit in each image. The competition evaluates your solution based on **categorization accuracy**.

---

## ğŸ“‚ Dataset Description

- **Train File**: `train.csv` â€“ contains **785 columns**:
  - `label`: the correct digit (0â€“9)
  - `pixel0` to `pixel783`: intensity of each pixel (values from 0 to 255)
  
- **Test File**: `test.csv` â€“ contains **784 columns**:
  - `pixel0` to `pixel783`: no label, your model will predict this

- Each image is **28x28 pixels**, unrolled into a 784-dimensional vector.

---

## ğŸ‘¶ Ideal Starting Point

This competition is ideal if:

- You know some **Python or R**
- You have **machine learning basics**
- You're **new to computer vision**

Itâ€™s a great opportunity to practice with:

- ğŸ§  **Neural networks** (basic and deep)
- ğŸ§ª **Dimensionality reduction** (PCA, Autoencoders)
- âš™ï¸ **ML techniques** like SVM, Random Forest, kNN
- ğŸ“Š **Data visualization** and segmentation

---

## ğŸ¯ Project-Specific Goals

This project is a comprehensive experimentation ground to:

- ğŸ” Reduce dimensionality with **PCA and Autoencoders**
- ğŸ§© Use **image segmentation** via clustering (k-means)
- âš™ï¸ Compare performance of **classical ML models** (SVC, RF, kNN)
- ğŸ§  Build **CNN-based deep learning** pipelines
- ğŸ”— Use **ensembling techniques** (voting, stacking) for better accuracy
- âš¡ Compare **model performance** with and without reduction
- ğŸ§ª Learn through **applied research**

---

## ğŸš€ Approach Overview

```text
1. Data Exploration
2. Dimensionality Reduction (PCA, Autoencoders)
3. Clustering / Segmentation (kMeans on pixels/features)
4. Traditional ML Models (SVC, RF)
5. CNN-based Deep Learning Models
6. Data Augmentation
7. Ensemble Strategies (Voting, Stacking)
8. Evaluation, Visualization, Saving Outputs
```

## ğŸ“Š Dimensionality Reduction Techniques

### ğŸŸ¢ PCA
- Reduce dimensionality while retaining **95â€“98% variance**.
- **Benefit**: Speeds up training for traditional ML models.

### ğŸ”µ Autoencoders
- Capture **non-linear feature representations**.
- **Benefit**: Often better than PCA for complex data.

---

## ğŸ§© Image Segmentation Strategy

We experiment with:

- **k-Means Clustering** on flattened pixel space or on extracted CNN features.
- Represent each image with **cluster memberships** or **cluster-based statistics**.
- Save transformed features to `.npy` or `.pkl` files for reuse.

---

## ğŸ¤– Machine Learning Models

| Model              | Input Type        | Notes                            |
|-------------------|-------------------|----------------------------------|
| **SVC**            | PCA / clustered   | Good baseline for clean data     |
| **Random Forest**  | PCA / clustered   | Handles noise well               |
| **CNN**            | Raw + augmented   | End-to-end learning              |
| **Autoencoder + ML**| Compressed vector | Dimensionality-aware combo       |

---

## ğŸ§¬ Data Augmentation (for CNNs)

- **Techniques**: Rotation, shifting, zooming, flipping  
- **Libraries**: `ImageDataGenerator`, `Albumentations`

---

## ğŸ§  Ensemble Techniques

### âœ… Soft Voting
- Blend predictions using **average of probabilities**.

### âœ… Stacking
- Use outputs of base models as input to a **meta-model** (e.g., Logistic Regression).

---

## ğŸ“ˆ Evaluation Metrics

- âœ… Accuracy on test set  
- âœ… Confusion matrix  
- âœ… Per-class performance  
- âœ… Training time vs performance comparison

---

## ğŸ“ Project Structure

```
digit-recognizer/
â”œâ”€â”€ data/                  # Dataset files (train.csv, test.csv, etc.)
â”œâ”€â”€ notebooks/             # Jupyter notebooks for each experiment
â”‚   â”œâ”€â”€ 01_explore_data.ipynb         # Initial EDA
â”‚   â”œâ”€â”€ 02_pca_classical_ml.ipynb     # PCA + SVC, RF
â”‚   â”œâ”€â”€ 03_autoencoder_ml.ipynb       # Autoencoder + SVC
â”‚   â”œâ”€â”€ 04_kmeans_features.ipynb      # kMeans segmentation + ML
â”‚   â”œâ”€â”€ 05_cnn_baseline.ipynb         # Baseline CNN
â”‚   â”œâ”€â”€ 06_cnn_augmented.ipynb        # CNN with augmentations
â”‚   â”œâ”€â”€ 07_ensemble_models.ipynb      # Voting/stacking ensembles
â”‚   â””â”€â”€ 08_visualizations.ipynb       # t-SNE, UMAP plots
â”œâ”€â”€ models/                # Saved models (Pickle, H5, etc.)
â”œâ”€â”€ outputs/               # Predictions and logs
â”œâ”€â”€ requirements.txt       # List of required packages
â””â”€â”€ README.md              # Project overview
```

---

## ğŸ§ª Planned Experiments

| Experiment                       | Status | Notes                                |
|----------------------------------|--------|--------------------------------------|
| Baseline CNN                     | â¬œï¸     |                                      |
| PCA + SVC                        | â¬œï¸     | Try 95% and 98% retained variance    |
| PCA + RF                         | â¬œï¸     | Check for overfitting                |
| Autoencoder + SVC                | â¬œï¸     | Compare vs PCA                       |
| kMeans Cluster Features + ML     | â¬œï¸     | Convert clusters into feature sets   |
| CNN with Augmentation            | â¬œï¸     | Improve generalization               |
| Stacked Model (SVC+RF+CNN)       | â¬œï¸     | Final ensemble                       |

---

## ğŸ›  Tools & Libraries Used

- **Python**, **NumPy**, **Pandas**, **Scikit-learn**  
- **TensorFlow / Keras**  
- **Matplotlib / Seaborn**  
- **UMAP / t-SNE** (for visualization)  
- **Gradio** (for interactive demos, optional)

---

## ğŸ§  Learnings & Reflections (To be updated)

- ğŸ“Œ Comparative impact of **PCA vs Autoencoder**  
- ğŸ“Œ When to prefer **classical ML over DL**  
- ğŸ“Œ Efficiency of **ensembles in small datasets**  
- ğŸ“Œ Real-world applications of **dimensionality reduction**

---

## ğŸ“œ License

**MIT License** â€“ Free to use with attribution.  
Digit data provided by [Kaggle](https://www.kaggle.com/competitions/digit-recognizer/overview).

---

## ğŸ‘¨â€ğŸ’» Author

- **Name**: Guna Venkat Doddi  
- **Project**: Part of `SSJ3-Kaggle-Projects` repository  
- **Contact**: [![GitHub - Guna Venkat Doddi](https://img.shields.io/badge/GitHub-Guna--Venkat--Doddi-black?logo=github&style=flat-square)](https://github.com/Guna-Venkat)s