# ğŸ¿ IMDB - Popcorn Sentiment Classification (NLP)

<p align="center">
  <a href="https://www.kaggle.com/competitions/word2vec-nlp-tutorial">
    <img src="https://img.shields.io/badge/Kaggle-Popcorn_Review_Classification-orange?style=for-the-badge&logo=kaggle&logoColor=white" alt="Kaggle Popcorn Sentiment"/>
  </a>
</p>

Welcome to **Popcorn NLP Sentiment Classification**, a classic binary text classification problem from the **IMDB Movie Review** dataset.  
This repository contains an end-to-end exploration using both traditional NLP and modern deep learning pipelines â€” part of the **SSJ3-ML-Journey**.

---

## ğŸ¯ Objective

Predict whether a movie review expresses **positive** or **negative** sentiment based on its text.

Youâ€™ll classify reviews using:

- ğŸ§  **Traditional models**: TF-IDF + Naive Bayes / SVM / Logistic Regression  
- ğŸ¤– **Optional upgrade**: Transformer-based models (RoBERTa / BERT)

---

## ğŸ“ Dataset Overview

| File                  | Description                              |
|-----------------------|------------------------------------------|
| `train.tsv`           | Contains `id`, `review`, and `sentiment` |
| `test.tsv`            | Contains `id` and `review`               |
| `sampleSubmission.csv`| Template for final predictions            |

- Reviews are raw HTML/text strings from IMDB movie reviews.
- **Labels**:  
  - `0` = Negative  
  - `1` = Positive

---

## ğŸ” Workflow

```
1. Load Data & Initial Inspection
2. Text Preprocessing (cleaning, stopwords, stemming)
3. Feature Extraction (TF-IDF, n-grams)
4. Traditional ML Modeling (NB, SVM, LR)
5. Evaluation & Visualization
6. Advanced Experiments (BERT/RoBERTa - optional)
```

## ğŸ§ª Observations

| Technique               | Accuracy | Notes                                      |
|-------------------------|----------|--------------------------------------------|
| TF-IDF + Naive Bayes    | ~0.87    | Simple, fast, and interpretable            |
| TF-IDF + SVM            | ~0.90    | Better generalization                      |
| TF-IDF + LogisticR      | ~0.89    | Stable baseline                            |
| Transformers (BERT)     | ~0.94+   | Requires more compute, better performance  |

## ğŸ“¦ Project Structure

```
popcorn_sentiment_nlp/
â”œâ”€â”€ data/                        # Raw and processed dataset
â”‚   â”œâ”€â”€ train.tsv
â”‚   â”œâ”€â”€ test.tsv
â”‚   â”œâ”€â”€ sampleSubmission.csv
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda_preprocessing.ipynb
â”‚   â”œâ”€â”€ 02_tfidf_logreg_nb_svm.ipynb
â”‚   â”œâ”€â”€ 03_transformer_finetune.ipynb  (optional)
â”‚   â”œâ”€â”€ 04_visualizations.ipynb
â”œâ”€â”€ models/                      # Trained model files (.pkl)
â”œâ”€â”€ outputs/                     # Submission files, reports
â”œâ”€â”€ visualizations/              # Wordclouds, confusion matrices
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ§¼ Text Cleaning & Preprocessing

- Remove HTML tags, punctuation  
- Lowercase conversion  
- Stopword removal (NLTK)  
- *Optional:* Stemming / Lemmatization  

```python
from nltk.corpus import stopwords
from bs4 import BeautifulSoup
import re

def clean_text(text):
    text = BeautifulSoup(text, "html.parser").get_text()
    text = re.sub("[^a-zA-Z]", " ", text).lower()
    text = " ".join([w for w in text.split() if w not in stopwords.words("english")])
    return text
```

# ğŸ”¤ Feature Engineering

| Vectorizer        | Settings                          |
|-------------------|-----------------------------------|
| TF-IDF            | `max_features=10000`, `ngram=(1,2)` |
| CountVectorizer   | *(optional)*                      |

```python
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=10000)
X = vectorizer.fit_transform(cleaned_reviews)
```

# ğŸ¤– Machine Learning Models

| Model              | Input     | Remarks                          |
|-------------------|-----------|----------------------------------|
| Naive Bayes       | TF-IDF    | Fast, interpretable              |
| Logistic Regression | TF-IDF  | Good for high-dim sparse         |
| SVM (Linear)      | TF-IDF    | Excellent decision boundary      |
| BERT / RoBERTa    | Raw text  | SOTA performance (optional)      |

---

# ğŸ“ˆ Evaluation Metrics

- Accuracy  
- F1 Score  
- Confusion Matrix  
- Training time  
- Misclassified examples  

---

# ğŸ“Š Visualizations

- Wordclouds (positive vs negative)  
- Confusion matrix  
- N-gram bar charts  
- Incorrect predictions samples  

# ğŸ§ª Optional Upgrades

- ğŸ” Try **Stemming** vs **Lemmatization**
- ğŸ” Interpret model coefficients (top positive/negative words)
- ğŸ“š Upgrade to **BERT** or **RoBERTa** (via HuggingFace)
- âš–ï¸ Use **class weights** or **balancing** for imbalanced datasets
- ğŸ§ª Use **GridSearchCV** for hyperparameter tuning

---

# ğŸ§  Learnings

- Text cleaning has a **huge impact** on model performance  
- **TF-IDF** is still powerful in classical ML pipelines  
- **BERT** offers massive performance gain at compute cost  
- Importance of **visualizing** and **interpreting** misclassifications  

## âœï¸ Author

- **Name**: Guna Venkat Doddi  
- **Project**: Part of `SSJ3-Kaggle-Projects` repository  
- **Contact**: [![GitHub - Guna Venkat Doddi](https://img.shields.io/badge/GitHub-Guna--Venkat--Doddi-black?logo=github&style=flat-square)](https://github.com/Guna-Venkat)

---