# ğŸ§  Common Machine Learning Workflow (SSJ3 Journey)

This document outlines the **common ML pipeline** steps shared across most structured datasets like **Titanic** (classification) and **House Prices** (regression). It serves as a base template before diverging into task-specific modeling.

---

## ğŸ—‚ï¸ 1. Data Loading

- Use `pandas.read_csv()` to load datasets.
- Check initial shape and use `.head()`, `.info()`, `.describe()` for quick exploration.

```python
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')
```

## ğŸ§¹ 2. Data Cleaning

- Identify and handle missing values.
- Convert data types appropriately  
  (e.g., `object â†’ category`, `str â†’ datetime`).
- Drop redundant or identifier columns  
  (e.g., `PassengerId`, `Id`).

---

## ğŸ” 3. Exploratory Data Analysis (EDA)

- Visualize target distribution using `seaborn` or `matplotlib`.
- Use `.value_counts()` or `.groupby()` to explore categorical features.
- Plot a correlation heatmap to identify multicollinearity.
- Detect outliers and highly skewed features.

---

## ğŸ§± 4. Feature Engineering

- Create new features:  
  `FamilySize = SibSp + Parch + 1`  
  `TotalSF = TotalBsmtSF + 1stFlrSF + 2ndFlrSF`
- Apply transformations for skewed features  
  (e.g., `log`, `Box-Cox`).
- Use domain knowledge to map ordinal categories  
  (e.g., `Ex: 5`, `Gd: 4`, ..., `None: 0`).

---

## ğŸ”  5. Encoding Categorical Features

- **Ordinal Encoding** â€“ for features with natural order  
  (e.g., `ExterQual`, `BsmtCond`)
- **Label Encoding** â€“ when meaningful numeric mapping exists.
- **OneHot Encoding** â€“ only when:
  - Feature has low cardinality
  - No natural ordering exists

â— Avoid OneHot for high-cardinality columns unless using tree-based models.

---

## âš–ï¸ 6. Feature Scaling

- Apply `StandardScaler` or `RobustScaler` for normalization.
- Not required for tree-based models like RandomForest or XGBoost.
- Always **scale after splitting** train/test data to avoid data leakage.

---

## ğŸ§ª 7. Model Building (Generic Flow)

```python
from sklearn.model_selection import train_test_split

X = train_df.drop('target', axis=1)
y = train_df['target']

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.1, stratify=y
)
```

## ğŸ”§ Algorithms

### ğŸ§  Classification Models
- `LogisticRegression`
- `RandomForestClassifier`
- `XGBoostClassifier`

### ğŸ“ Regression Models
- `LinearRegression`
- `Lasso`
- `Ridge`
- `XGBoostRegressor`

---

## ğŸ“Š 8. Evaluation Metrics

### ğŸ“ˆ Regression Metrics

| Metric | Description                  |
|--------|------------------------------|
| MAE    | Mean Absolute Error          |
| RMSE   | Root Mean Squared Error      |
| RÂ²     | Coefficient of Determination |

### ğŸ“‰ Classification Metrics

| Metric     | Description                              |
|------------|------------------------------------------|
| Accuracy   | % of correct predictions                 |
| Precision  | TP / (TP + FP)                           |
| Recall     | TP / (TP + FN)                           |
| F1 Score   | Harmonic mean of precision & recall      |
| ROC-AUC    | Area under the ROC curve (probabilities) |

---

## ğŸ§  Model Interpretability (Optional)

- `.coef_` â€“ for linear models  
- `.feature_importances_` â€“ for tree-based models  
- `permutation_importance` â€“ model-agnostic  
- **SHAP** â€“ advanced interpretability (e.g., XGBoost, LightGBM)

---

## âœ… Checklist

- [x] Missing values handled
- [x] Categorical data encoded
- [x] Numeric features scaled
- [x] Evaluation metrics computed
- [x] Hyperparameters tuned

---

## ğŸ“ Notes

- Logic is largely reusable across structured datasets like Titanic and House Prices.
- Final **submission format** depends on problem type:
  - **Classification** â†’ predicted class label
  - **Regression** â†’ numeric prediction value
- Avoid overfitting via:
  - Regularization (`Ridge`, `Lasso`)
  - Tree pruning or max depth
  - Cross-validation
  - Early stopping (for boosting models)
